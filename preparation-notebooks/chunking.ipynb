{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the data after correcting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train a tokenizer with the cleaned corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output folder will be the same, so the previous tokenizer will be overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santi\\AppData\\Local\\Temp\\ipykernel_31224\\2841662301.py:6: DtypeWarning: Columns (1,2,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"../data/old-spanish-corpus-cleaned.tsv\", sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 52000\n",
    "TRAINING_BATCH_SIZE = 1000\n",
    "HF_CHECKPOINT = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
    "\n",
    "pretrained_tokenizer = AutoTokenizer.from_pretrained(HF_CHECKPOINT)\n",
    "df = pd.read_csv(\"../data/old-spanish-corpus-cleaned.tsv\", sep='\\t')\n",
    "\n",
    "training_corpus = (\n",
    "    df.loc[i:i+TRAINING_BATCH_SIZE, \"text\"].astype(str) \n",
    "    for i in range(0, len(df), TRAINING_BATCH_SIZE)\n",
    ")\n",
    "\n",
    "tokenizer = pretrained_tokenizer.train_new_from_iterator(training_corpus, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE: ['CON', '##QU', '##ISTA', '4', '*', '*', 'cu', '##tar', 'lo', 'que', 'fuese', 'de', 'su', 'mayor', 'agrado', ',', 'sin', 'dis', '\"', 'curr']\n",
      "AFTER: ['CONQUISTA', '4', '*', '*', 'cu', '##tar', 'lo', 'que', 'fuese', 'de', 'su', 'mayor', 'agrado', ',', 'sin', 'dis', '\"', 'cur', '##rir', 'en']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../output/tokenizer\\\\tokenizer_config.json',\n",
       " '../output/tokenizer\\\\special_tokens_map.json',\n",
       " '../output/tokenizer\\\\vocab.txt',\n",
       " '../output/tokenizer\\\\added_tokens.json',\n",
       " '../output/tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = df.loc[5, \"text\"]\n",
    "print(\"BEFORE:\", pretrained_tokenizer.tokenize(example)[:20])\n",
    "print(\"AFTER:\", tokenizer.tokenize(example)[:20])\n",
    "tokenizer.save_pretrained(\"../output/tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chunk the texts that have more than 512 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "512 is the maximum number of tokens that can be processed by BERT-like models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Chunk all the texts that have more than 512 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS_LENGTH = 256 # 512/2, a number much below 512 so that it will still fit for different tokenizers\n",
    "\n",
    "def chunk(text):\n",
    "    chunks = []\n",
    "    sentences = nltk.sent_tokenize(text) # text.split('.'), but enhanced\n",
    "    current_chunk = sentences[0]\n",
    "    for sentence in sentences[1:]:\n",
    "        new_chunk_tks = len(tokenizer(f\"{current_chunk} {sentence}\")['input_ids'])\n",
    "        if (new_chunk_tks) > (MAX_TOKENS_LENGTH-2):\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sentence\n",
    "        else:\n",
    "            current_chunk = f\"{current_chunk} {sentence}\"\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked 0/1233487 (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (694 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked 100000/1233487 (8.11%)\n",
      "Chunked 200000/1233487 (16.21%)\n",
      "Chunked 300000/1233487 (24.32%)\n",
      "Chunked 400000/1233487 (32.43%)\n",
      "Chunked 500000/1233487 (40.54%)\n",
      "Chunked 600000/1233487 (48.64%)\n",
      "Chunked 700000/1233487 (56.75%)\n",
      "Chunked 800000/1233487 (64.86%)\n",
      "Chunked 900000/1233487 (72.96%)\n",
      "Chunked 1000000/1233487 (81.07%)\n",
      "Chunked 1100000/1233487 (89.18%)\n",
      "Chunked 1200000/1233487 (97.29%)\n",
      "Finished 100%\n"
     ]
    }
   ],
   "source": [
    "idx_max = dict()\n",
    "for i, sentence in enumerate(df['text'].astype(str)):\n",
    "    num_tokens = len(tokenizer(sentence)['input_ids'])\n",
    "    if num_tokens > MAX_TOKENS_LENGTH:\n",
    "        chunks = chunk(sentence)\n",
    "        idx_max[i] = chunks\n",
    "        # print(f\"{i}. {num_tokens} tokens -> {len(chunks)} chunks\")\n",
    "        # print(\"Original:\")\n",
    "        # print(sentence)\n",
    "        # print(\"Chunks: [\")\n",
    "        # [print(len(tokenizer(j)['input_ids']), j) for j in chunks]\n",
    "        # print(\"]\")\n",
    "    if i % 100000 == 0:\n",
    "        print(f\"Chunked {i}/{len(df)} ({i/len(df):.2%})\")\n",
    "\n",
    "print(\"Finished 100%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.loc[list(idx_max.keys())[0], \"text\"][:10] == idx_max[list(idx_max.keys())[0]][0][:10], \"Texts do not coincide\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/chunkedTexts.json\", \"w\") as json_file:\n",
    "    json.dump(idx_max, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Update the original DataFrame with the chunked texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/chunkedTexts.json\", \"r\") as json_file:\n",
    "    idx_max = json.load(json_file)\n",
    "idx_max = {int(k): v for k, v in idx_max.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Added 485510 chunks (39.36%)\n"
     ]
    }
   ],
   "source": [
    "original_length = len(df)\n",
    "updated_length = len(df)+sum([len(i)-1 for i in idx_max.values()])\n",
    "print(f\"\\nAdded {updated_length - original_length} chunks ({updated_length / original_length - 1:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1/1718997 (0.00%)\n",
      "Added 282168/1718997 (16.41%)\n",
      "Added 528719/1718997 (30.76%)\n",
      "Added 741572/1718997 (43.14%)\n",
      "Added 844939/1718997 (49.15%)\n",
      "Added 946264/1718997 (55.05%)\n",
      "Added 1053346/1718997 (61.28%)\n",
      "Added 1155481/1718997 (67.22%)\n",
      "Added 1259787/1718997 (73.29%)\n",
      "Added 1368137/1718997 (79.59%)\n",
      "Added 1474160/1718997 (85.76%)\n",
      "Added 1579286/1718997 (91.87%)\n",
      "Added 1684382/1718997 (97.99%)\n",
      "Finished 100%\n"
     ]
    }
   ],
   "source": [
    "finaldf = []\n",
    "for idx, row in df.iterrows():\n",
    "    if idx in idx_max:\n",
    "        cI = 0\n",
    "        for t in idx_max[idx]:\n",
    "            new_row = row.copy()\n",
    "            new_row['text'] = t\n",
    "            new_row['chunk_id'] = cI\n",
    "            finaldf.append(new_row)\n",
    "            cI += 1\n",
    "    else:\n",
    "        row['chunk_id'] = 0\n",
    "        finaldf.append(row)\n",
    "    if idx % 100000 == 0:\n",
    "        print(f\"Added {len(finaldf)}/{updated_length} ({len(finaldf)/updated_length:.2%})\")\n",
    "\n",
    "print(\"Finished 100%\")\n",
    "assert len(finaldf) == updated_length, f\"Lengths must match, expected length to be {updated_length}, but was {len(finaldf)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>source_id</th>\n",
       "      <th>source_text_id</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>place</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The British Library</td>\n",
       "      <td>3436138</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>Historia de la conquista de México, etc [With ...</td>\n",
       "      <td>1809</td>\n",
       "      <td>London</td>\n",
       "      <td>HISTORIA DE LA CONQUISTA PE MÉXICO, POBLACIÓN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The British Library</td>\n",
       "      <td>3436138</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>Historia de la conquista de México, etc [With ...</td>\n",
       "      <td>1809</td>\n",
       "      <td>London</td>\n",
       "      <td>This Work, as well as LAS FÁBULAS LITERARIAS, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The British Library</td>\n",
       "      <td>3436138</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>Historia de la conquista de México, etc [With ...</td>\n",
       "      <td>1809</td>\n",
       "      <td>London</td>\n",
       "      <td>HISTORIA De la Conquista, población y Progreso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The British Library</td>\n",
       "      <td>3436138</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>Historia de la conquista de México, etc [With ...</td>\n",
       "      <td>1809</td>\n",
       "      <td>London</td>\n",
       "      <td>CONQUISTA quartel observando la batalla, y rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The British Library</td>\n",
       "      <td>3436138</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>Historia de la conquista de México, etc [With ...</td>\n",
       "      <td>1809</td>\n",
       "      <td>London</td>\n",
       "      <td>Pon deró con afectada seguridad el atrevimient...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1718992</th>\n",
       "      <td>Project Gutenberg</td>\n",
       "      <td>53294</td>\n",
       "      <td>916</td>\n",
       "      <td>0</td>\n",
       "      <td>Recuerdos Del Tiempo Viejo</td>\n",
       "      <td>1817-1893</td>\n",
       "      <td>?</td>\n",
       "      <td>Pero amo á Barcelona por tiranía     de ley in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1718993</th>\n",
       "      <td>Project Gutenberg</td>\n",
       "      <td>53294</td>\n",
       "      <td>917</td>\n",
       "      <td>0</td>\n",
       "      <td>Recuerdos Del Tiempo Viejo</td>\n",
       "      <td>1817-1893</td>\n",
       "      <td>?</td>\n",
       "      <td>Barcelona trabaja... y á su existencia     el ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1718994</th>\n",
       "      <td>Project Gutenberg</td>\n",
       "      <td>53294</td>\n",
       "      <td>919</td>\n",
       "      <td>0</td>\n",
       "      <td>Recuerdos Del Tiempo Viejo</td>\n",
       "      <td>1817-1893</td>\n",
       "      <td>?</td>\n",
       "      <td>Olvidaba que entre ambas hay diferencia:     n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1718995</th>\n",
       "      <td>Project Gutenberg</td>\n",
       "      <td>53294</td>\n",
       "      <td>920</td>\n",
       "      <td>0</td>\n",
       "      <td>Recuerdos Del Tiempo Viejo</td>\n",
       "      <td>1817-1893</td>\n",
       "      <td>?</td>\n",
       "      <td>La diferencia es esta: pero es preciso     que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1718996</th>\n",
       "      <td>Project Gutenberg</td>\n",
       "      <td>53294</td>\n",
       "      <td>921</td>\n",
       "      <td>0</td>\n",
       "      <td>Recuerdos Del Tiempo Viejo</td>\n",
       "      <td>1817-1893</td>\n",
       "      <td>?</td>\n",
       "      <td>_Esta obra es propiedad de su Autor, el que pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1718997 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source source_id source_text_id  chunk_id  \\\n",
       "0        The British Library   3436138              9         0   \n",
       "1        The British Library   3436138             10         0   \n",
       "2        The British Library   3436138             11         0   \n",
       "3        The British Library   3436138             12         0   \n",
       "4        The British Library   3436138             12         1   \n",
       "...                      ...       ...            ...       ...   \n",
       "1718992    Project Gutenberg     53294            916         0   \n",
       "1718993    Project Gutenberg     53294            917         0   \n",
       "1718994    Project Gutenberg     53294            919         0   \n",
       "1718995    Project Gutenberg     53294            920         0   \n",
       "1718996    Project Gutenberg     53294            921         0   \n",
       "\n",
       "                                                     title       date   place  \\\n",
       "0        Historia de la conquista de México, etc [With ...       1809  London   \n",
       "1        Historia de la conquista de México, etc [With ...       1809  London   \n",
       "2        Historia de la conquista de México, etc [With ...       1809  London   \n",
       "3        Historia de la conquista de México, etc [With ...       1809  London   \n",
       "4        Historia de la conquista de México, etc [With ...       1809  London   \n",
       "...                                                    ...        ...     ...   \n",
       "1718992                         Recuerdos Del Tiempo Viejo  1817-1893       ?   \n",
       "1718993                         Recuerdos Del Tiempo Viejo  1817-1893       ?   \n",
       "1718994                         Recuerdos Del Tiempo Viejo  1817-1893       ?   \n",
       "1718995                         Recuerdos Del Tiempo Viejo  1817-1893       ?   \n",
       "1718996                         Recuerdos Del Tiempo Viejo  1817-1893       ?   \n",
       "\n",
       "                                                      text  \n",
       "0        HISTORIA DE LA CONQUISTA PE MÉXICO, POBLACIÓN ...  \n",
       "1        This Work, as well as LAS FÁBULAS LITERARIAS, ...  \n",
       "2        HISTORIA De la Conquista, población y Progreso...  \n",
       "3        CONQUISTA quartel observando la batalla, y rec...  \n",
       "4        Pon deró con afectada seguridad el atrevimient...  \n",
       "...                                                    ...  \n",
       "1718992  Pero amo á Barcelona por tiranía     de ley in...  \n",
       "1718993  Barcelona trabaja... y á su existencia     el ...  \n",
       "1718994  Olvidaba que entre ambas hay diferencia:     n...  \n",
       "1718995  La diferencia es esta: pero es preciso     que...  \n",
       "1718996  _Esta obra es propiedad de su Autor, el que pe...  \n",
       "\n",
       "[1718997 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newcols = list(df.columns)\n",
    "newcols.insert(3, \"chunk_id\")\n",
    "finaldf = pd.DataFrame(finaldf, columns=newcols)\n",
    "finaldf = finaldf.reset_index(drop=True)\n",
    "\n",
    "finaldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldf.to_csv(\"../data/old-spanish-corpus-chunked.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = finaldf['text'].apply(lambda x: len(tokenizer.tokenize(x))).sum()\n",
    "total_words = finaldf['text'].apply(lambda x: len(x.split())).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset has:\n",
      "\ttokens: 193,828,170\n",
      "\twords: 149,299,085\n",
      "\ttexts: 1,718,997\n"
     ]
    }
   ],
   "source": [
    "print(f\"New dataset has:\\n\\ttokens: {total_tokens:,}\\n\\twords: {total_words:,}\\n\\ttexts: {len(finaldf):,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
