{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the data after correcting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train a tokenizer with the cleaned corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output folder will be the same, so the previous tokenizer will be overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santi\\AppData\\Local\\Temp\\ipykernel_1504\\2841662301.py:6: DtypeWarning: Columns (1,2,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"../data/old-spanish-corpus-cleaned.tsv\", sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 52000\n",
    "TRAINING_BATCH_SIZE = 1000\n",
    "HF_CHECKPOINT = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
    "\n",
    "pretrained_tokenizer = AutoTokenizer.from_pretrained(HF_CHECKPOINT)\n",
    "df = pd.read_csv(\"../data/old-spanish-corpus-cleaned.tsv\", sep='\\t')\n",
    "\n",
    "training_corpus = (\n",
    "    df.loc[i:i+TRAINING_BATCH_SIZE, \"text\"].astype(str) \n",
    "    for i in range(0, len(df), TRAINING_BATCH_SIZE)\n",
    ")\n",
    "\n",
    "tokenizer = pretrained_tokenizer.train_new_from_iterator(training_corpus, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE: ['CON', '##QU', '##ISTA', '4', '*', '*', 'cu', '##tar', 'lo', 'que', 'fuese', 'de', 'su', 'mayor', 'agrado', ',', 'sin', 'dis', '\"', 'curr']\n",
      "AFTER: ['CONQUISTA', '4', '*', '*', 'cu', '##tar', 'lo', 'que', 'fuese', 'de', 'su', 'mayor', 'agrado', ',', 'sin', 'dis', '\"', 'cur', '##rir', 'en']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../output/tokenizer\\\\tokenizer_config.json',\n",
       " '../output/tokenizer\\\\special_tokens_map.json',\n",
       " '../output/tokenizer\\\\vocab.txt',\n",
       " '../output/tokenizer\\\\added_tokens.json',\n",
       " '../output/tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = df.loc[5, \"text\"]\n",
    "print(\"BEFORE:\", pretrained_tokenizer.tokenize(example)[:20])\n",
    "print(\"AFTER:\", tokenizer.tokenize(example)[:20])\n",
    "tokenizer.save_pretrained(\"../output/tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chunk the texts that have more than 512 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "512 is the maximum number of tokens that can be processed by BERT-like models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Chunk all the texts that have more than 512 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS_LENGTH = 384 # 512*3/4, a number much below 512 so that it will still fit for different tokenizers\n",
    "\n",
    "def chunk(text):\n",
    "    chunks = []\n",
    "    sentences = nltk.sent_tokenize(text) # text.split('.'), but enhanced\n",
    "    current_chunk = sentences[0]\n",
    "    for sentence in sentences[1:]:\n",
    "        new_chunk_tks = len(tokenizer(f\"{current_chunk} {sentence}\")['input_ids'])\n",
    "        if (new_chunk_tks) > (MAX_TOKENS_LENGTH-2):\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sentence\n",
    "        else:\n",
    "            current_chunk = f\"{current_chunk} {sentence}\"\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked 0/1233487 (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (694 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "idx_max = dict()\n",
    "for i, sentence in enumerate(df['text'].astype(str)):\n",
    "    num_tokens = len(tokenizer(sentence)['input_ids'])\n",
    "    if num_tokens > MAX_TOKENS_LENGTH:\n",
    "        chunks = chunk(sentence)\n",
    "        idx_max[i] = chunks\n",
    "        # print(f\"{i}. {num_tokens} tokens -> {len(chunks)} chunks\")\n",
    "        # print(\"Original:\")\n",
    "        # print(sentence)\n",
    "        # print(\"Chunks: [\")\n",
    "        # [print(len(tokenizer(j)['input_ids']), j) for j in chunks]\n",
    "        # print(\"]\")\n",
    "    if i % 100000 == 0:\n",
    "        print(f\"Chunked {i}/{len(df)} ({i/len(df):.2%})\")\n",
    "\n",
    "print(\"Finished 100%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Added 235339 chunks (18.34%)\n"
     ]
    }
   ],
   "source": [
    "original_length = len(df)\n",
    "updated_length = len(df)+sum([len(i)-1 for i in idx_max.values()])\n",
    "print(f\"\\nAdded {updated_length - original_length} chunks ({updated_length / original_length - 1:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.loc[list(idx_max.keys())[0], \"text\"][:10] == idx_max[list(idx_max.keys())[0]][0][:10], \"Texts do not coincide\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/chunkedTexts.json\", \"w\") as json_file:\n",
    "    json.dump(idx_max, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Update the original DataFrame with the chunked texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/chunkedTexts.json\", \"r\") as json_file:\n",
    "    idx_max = json.load(json_file)\n",
    "idx_max = {int(k): v for k, v in idx_max.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1/1518717 (0.00%)\n",
      "Added 195369/1518717 (12.86%)\n",
      "Added 367371/1518717 (24.19%)\n",
      "Added 520011/1518717 (34.24%)\n",
      "Added 621072/1518717 (40.89%)\n",
      "Added 721348/1518717 (47.50%)\n",
      "Added 823074/1518717 (54.20%)\n",
      "Added 924267/1518717 (60.86%)\n",
      "Added 1025951/1518717 (67.55%)\n",
      "Added 1128348/1518717 (74.30%)\n",
      "Added 1231229/1518717 (81.07%)\n",
      "Added 1332768/1518717 (87.76%)\n",
      "Added 1434333/1518717 (94.44%)\n",
      "Finished 100%\n"
     ]
    }
   ],
   "source": [
    "finaldf = []\n",
    "for idx, row in df.iterrows():\n",
    "    if idx in idx_max:\n",
    "        cI = 0\n",
    "        for t in idx_max[idx]:\n",
    "            new_row = row.copy()\n",
    "            new_row['text'] = t\n",
    "            new_row['chunk_id'] = cI\n",
    "            finaldf.append(new_row)\n",
    "            cI += 1\n",
    "    else:\n",
    "        row['chunk_id'] = 0\n",
    "        finaldf.append(row)\n",
    "    if idx % 100000 == 0:\n",
    "        print(f\"Added {len(finaldf)}/{updated_length} ({len(finaldf)/updated_length:.2%})\")\n",
    "\n",
    "print(\"Finished 100%\")\n",
    "assert len(finaldf) == updated_length, f\"Lengths must match, expected length to be {updated_length}, but was {len(finaldf)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>source_id</th>\n",
       "      <th>source_id_text</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>place</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The British Library</td>\n",
       "      <td>3436138</td>\n",
       "      <td>9</td>\n",
       "      <td>Historia de la conquista de México, etc [With ...</td>\n",
       "      <td>1809-01-01 00:00:00</td>\n",
       "      <td>London</td>\n",
       "      <td>HISTORIA DE LA CONQUISTA PE MÉXICO, POBLACIÓN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The British Library</td>\n",
       "      <td>3436138</td>\n",
       "      <td>10</td>\n",
       "      <td>Historia de la conquista de México, etc [With ...</td>\n",
       "      <td>1809-01-01 00:00:00</td>\n",
       "      <td>London</td>\n",
       "      <td>This Work, as well as LAS FÁBULAS LITERARIAS, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The British Library</td>\n",
       "      <td>3436138</td>\n",
       "      <td>11</td>\n",
       "      <td>Historia de la conquista de México, etc [With ...</td>\n",
       "      <td>1809-01-01 00:00:00</td>\n",
       "      <td>London</td>\n",
       "      <td>HISTORIA De la Conquista, población y Progreso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The British Library</td>\n",
       "      <td>3436138</td>\n",
       "      <td>12</td>\n",
       "      <td>Historia de la conquista de México, etc [With ...</td>\n",
       "      <td>1809-01-01 00:00:00</td>\n",
       "      <td>London</td>\n",
       "      <td>CONQUISTA quartel observando la batalla, y rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The British Library</td>\n",
       "      <td>3436138</td>\n",
       "      <td>13</td>\n",
       "      <td>Historia de la conquista de México, etc [With ...</td>\n",
       "      <td>1809-01-01 00:00:00</td>\n",
       "      <td>London</td>\n",
       "      <td>DE NUEVA ESPAÑA. 3 \" pretexto á los sediciosos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518712</th>\n",
       "      <td>Project Gutenberg</td>\n",
       "      <td>53294</td>\n",
       "      <td>917</td>\n",
       "      <td>Recuerdos Del Tiempo Viejo</td>\n",
       "      <td>1817-1893</td>\n",
       "      <td>?</td>\n",
       "      <td>Barcelona trabaja... y á su existencia  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518713</th>\n",
       "      <td>Project Gutenberg</td>\n",
       "      <td>53294</td>\n",
       "      <td>919</td>\n",
       "      <td>Recuerdos Del Tiempo Viejo</td>\n",
       "      <td>1817-1893</td>\n",
       "      <td>?</td>\n",
       "      <td>Olvidaba que entre ambas hay diferencia:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518714</th>\n",
       "      <td>Project Gutenberg</td>\n",
       "      <td>53294</td>\n",
       "      <td>920</td>\n",
       "      <td>Recuerdos Del Tiempo Viejo</td>\n",
       "      <td>1817-1893</td>\n",
       "      <td>?</td>\n",
       "      <td>La diferencia es esta: pero es preciso  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518715</th>\n",
       "      <td>Project Gutenberg</td>\n",
       "      <td>53294</td>\n",
       "      <td>921</td>\n",
       "      <td>Recuerdos Del Tiempo Viejo</td>\n",
       "      <td>1817-1893</td>\n",
       "      <td>?</td>\n",
       "      <td>_Esta obra es propiedad de su Autor, el que pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518716</th>\n",
       "      <td>Project Gutenberg</td>\n",
       "      <td>53294</td>\n",
       "      <td>922</td>\n",
       "      <td>Recuerdos Del Tiempo Viejo</td>\n",
       "      <td>1817-1893</td>\n",
       "      <td>?</td>\n",
       "      <td>End of Project Gutenberg's Recuerdos Del Tiemp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1518717 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source source_id source_id_text  \\\n",
       "0        The British Library   3436138              9   \n",
       "1        The British Library   3436138             10   \n",
       "2        The British Library   3436138             11   \n",
       "3        The British Library   3436138             12   \n",
       "4        The British Library   3436138             13   \n",
       "...                      ...       ...            ...   \n",
       "1518712    Project Gutenberg     53294            917   \n",
       "1518713    Project Gutenberg     53294            919   \n",
       "1518714    Project Gutenberg     53294            920   \n",
       "1518715    Project Gutenberg     53294            921   \n",
       "1518716    Project Gutenberg     53294            922   \n",
       "\n",
       "                                                     title  \\\n",
       "0        Historia de la conquista de México, etc [With ...   \n",
       "1        Historia de la conquista de México, etc [With ...   \n",
       "2        Historia de la conquista de México, etc [With ...   \n",
       "3        Historia de la conquista de México, etc [With ...   \n",
       "4        Historia de la conquista de México, etc [With ...   \n",
       "...                                                    ...   \n",
       "1518712                         Recuerdos Del Tiempo Viejo   \n",
       "1518713                         Recuerdos Del Tiempo Viejo   \n",
       "1518714                         Recuerdos Del Tiempo Viejo   \n",
       "1518715                         Recuerdos Del Tiempo Viejo   \n",
       "1518716                         Recuerdos Del Tiempo Viejo   \n",
       "\n",
       "                        date   place  \\\n",
       "0        1809-01-01 00:00:00  London   \n",
       "1        1809-01-01 00:00:00  London   \n",
       "2        1809-01-01 00:00:00  London   \n",
       "3        1809-01-01 00:00:00  London   \n",
       "4        1809-01-01 00:00:00  London   \n",
       "...                      ...     ...   \n",
       "1518712            1817-1893       ?   \n",
       "1518713            1817-1893       ?   \n",
       "1518714            1817-1893       ?   \n",
       "1518715            1817-1893       ?   \n",
       "1518716            1817-1893       ?   \n",
       "\n",
       "                                                      text  \n",
       "0        HISTORIA DE LA CONQUISTA PE MÉXICO, POBLACIÓN ...  \n",
       "1        This Work, as well as LAS FÁBULAS LITERARIAS, ...  \n",
       "2        HISTORIA De la Conquista, población y Progreso...  \n",
       "3        CONQUISTA quartel observando la batalla, y rec...  \n",
       "4        DE NUEVA ESPAÑA. 3 \" pretexto á los sediciosos...  \n",
       "...                                                    ...  \n",
       "1518712        Barcelona trabaja... y á su existencia  ...  \n",
       "1518713        Olvidaba que entre ambas hay diferencia:...  \n",
       "1518714        La diferencia es esta: pero es preciso  ...  \n",
       "1518715  _Esta obra es propiedad de su Autor, el que pe...  \n",
       "1518716  End of Project Gutenberg's Recuerdos Del Tiemp...  \n",
       "\n",
       "[1518717 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaldf = pd.DataFrame(finaldf, columns=df.columns)\n",
    "finaldf = finaldf.reset_index(drop=True)\n",
    "finaldf = finaldf.reindex(columns=['source', 'source_id', 'source_text_id', 'chunk_id', 'title', 'date', 'place', 'text'])\n",
    "\n",
    "finaldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldf.to_csv(\"../data/old-spanish-corpus-chunked.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset has:\n",
      "\ttokens: 195,086,372\n",
      "\t   texts: 1,518,717\n"
     ]
    }
   ],
   "source": [
    "total_tokens = finaldf['text'].apply(lambda x: len(tokenizer.tokenize(x))).sum()\n",
    "print(f\"New dataset has:\\n\\ttokens: {total_tokens:,}\\n\\t   texts: {updated_length:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
