{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model fine-tuning using MLM task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAILABLE_GPU = 2 # Available GPU with 0% usage\n",
    "HUGGINGFACE_MODEL = \"dccuchile/bert-base-spanish-wwm-uncased\" # HuggingFace pre-trained model to train\n",
    "MODEL_SAVE_PATH = \"./output/old-spanish-beto-uncased.pt\" # Path to save the trained model\n",
    "\n",
    "MASK_PROB = 0.15 # Probability of masking within a text\n",
    "LR = 2e-5 # Learning rate for Adam\n",
    "EPOCHS = 5 # Number of epochs to train\n",
    "BATCH_SIZE = 16 # Batch size for training\n",
    "MAX_TOKENIZER_LENGTH = 512 # Maximum length of the texts in the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/historynlp/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/historynlp/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{AVAILABLE_GPU}\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "tf_device=f'/gpu:{AVAILABLE_GPU}'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load pre-trained LM for MLM and it's tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the tokenizer and the model, using the Model's `transformers` ID from Hugging Face. It's important to consider the possible differences between _cased_ and _uncased_ models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(HUGGINGFACE_MODEL)\n",
    "model = AutoModelForMaskedLM.from_pretrained(HUGGINGFACE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5, 1, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLS_TOKEN = tokenizer.cls_token_id\n",
    "SEP_TOKEN = tokenizer.sep_token_id\n",
    "PAD_TOKEN = tokenizer.pad_token_id\n",
    "MASK_TOKEN = tokenizer.mask_token_id\n",
    "CLS_TOKEN, SEP_TOKEN, PAD_TOKEN, MASK_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and pre-process dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training the model, we'll take the training split from the full spanish corpus, after cleaning. Given after the preparation stage:\n",
    "> This corpus is already chunked for no more than 512 tokens, so there will be no chunking required for models that has this maximum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/old-spanish-corpus-chunked.tsv\", sep=\"\\t\", usecols=[\"text\"])\n",
    "df = df[(df.source != \"19th century Latam Newspapers\")]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random sample of a text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--Tal vez se trate de una simple filtración--dijo Van-Horn--. Tenemos bomba a bordo y luego la haremos funcionar.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[10000][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenize and Mask the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's possible to start the retraining process of the model for the MLM task. For that, we'll first tokenize the input texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture tokenizer_output\n",
    "%%time\n",
    "\n",
    "# All the texts of our dataset are stored in dataset[\"text\"]\n",
    "inputs = tokenizer(dataset[\"text\"], return_tensors='pt', max_length=MAX_TOKENIZER_LENGTH, truncation=True, padding='max_length')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 59s, sys: 2min 47s, total: 16min 46s\n",
      "Wall time: 4min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    4,  1098,  1000,  ...,     1,     1,     1],\n",
       "        [    4,  1413,   998,  ...,     1,     1,     1],\n",
       "        [    4,  1032,  2168,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    4,  1057,  9447,  ...,     1,     1,     1],\n",
       "        [    4,  1032,  3876,  ...,     1,     1,     1],\n",
       "        [    4,  1265, 30957,  ...,     1,     1,     1]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a comparation between the masked sentence and the original one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Comenzóseme a hacer áspera la morada y desapacibles los zaguanes. Fuí entrando poco a poco entre unos sastres que se me llegaron, que iban medrosos de los diablos. En la primera entrada hallamos siete demonios escribiendo los que íbamos entrando. Preguntáronme mi nombre; díjele y pasé. Llegaron a mis compañeros, y dijeron que eran remendones, y dijo uno de los diablos: «Deben entender los remendones en el mundo que no se hizo el infierno sino para ellos, según se vienen por acá.» Preguntó otro diablo cuántos eran; respondieron que ciento, y replicó un verdugo mal barbado entrecano: «¿Ciento, y sastres? No pueden ser tan pocos; la menor partida que habemos recibido ha sido de mil y ochocientos. En verdad que estamos por no recibirles.» Afligiéronse ellos; mas al fin entraron. Ved cuáles son los malos, que es para ellos amenaza el no dejarlos entrar en el infierno. Entró el primero[595] un negro, chiquito, rubio, de mal pelo; dió un salto en viéndose allá, y dijo: «Ahora acá estamos todos.» Salió de un lugar, donde estaba aposentado, un diablo de marca mayor[596], corcovado y cojo; y arrojándolos en una hondura muy grande, dijo: «Allá va leña.» Por curiosidad me llegué a él y le pregunté de qué estaba corcovado y cojo, y me dijo (que era diablo de pocas palabras): «Yo era recuero de remendones. Iba por ellos al mundo, y de traerlos a cuestas me hice corcovado y cojo; he dado en la cuenta, y hallo que se vienen mucho más apriesa que yo los puedo traer.» En esto hizo otro vómito dellos el mundo, y hube de entrarme porque no había donde estar ya allí, y el monstruo infernal empezó a traspalar, y diz que es la mejor leña que se quema en el infierno, remendones de todo oficio, gente que sólo tiene bueno ser enemiga de novedades.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[100]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] comenzó ##se ##me a hacer ás ##pera la morada y desa ##pac ##ibles los za ##gua ##nes . fuí entrando poco a poco entre unos sastre ##s que se me llegaron , que iban med ##rosos de los diablos . en la primera entrada hallamos siete demonios escribiendo los que íbamos entrando . pregun ##tár ##on ##me mi nombre ; dí ##je ##le y pasé . llegaron a mis compañeros , y dijeron que eran rem ##endo ##nes , y dijo uno de los diablos : [UNK] deben entender los rem ##endo ##nes en el mundo que no se hizo el infierno sino para ellos , según se vienen por acá . [UNK] preguntó otro diablo cuántos eran ; respondieron que ciento , y replic ##ó un verdu ##go mal barba ##do entre ##cano : [UNK] ¿ ciento , y sastre ##s ? no pueden ser tan pocos ; la menor partida que hab ##emos recibido ha sido de mil y ocho ##cientos . en verdad que estamos por no recibir ##les . [UNK] aflig ##i ##éro ##ns ##e ellos ; mas al fin entraron . ve ##d cuáles son los malos , que es para ellos amenaza el no dejarlos entrar en el infierno . entró el primero [ 59 ##5 ] un negro , chi ##quito , rubio , de mal pelo ; dió un salto en vi ##éndose allá , y dijo : [UNK] ahora acá estamos todos . [UNK] salió de un lugar , donde estaba apos ##entado , un diablo de marca mayor [ 59 ##6 ] , cor ##co ##vado y cojo ; y arro ##ján ##dol ##os en una hon ##dura muy grande , dijo : [UNK] allá va leña . [UNK] por curiosidad me llegué a él y le pregunté de qué estaba cor ##co ##vado y cojo , y me dijo ( que era diablo de pocas palabras ) : [UNK] yo era recuer ##o de rem ##endo ##nes . iba por ellos al mundo , y de traerlo ##s a cuesta ##s me hice cor ##co ##vado y cojo ; he dado en la cuenta , y hallo que se vienen mucho más aprie ##sa que yo los puedo traer . [UNK] en esto hizo otro vómito dell ##os el mundo , y hub ##e de entrar ##me porque no había donde estar ya allí , y el monstruo infernal empezó a tras ##pal ##ar , y di ##z que es la mejor leña que se quema en el infierno , rem ##endo ##nes de todo oficio , gente que sólo tiene bueno ser enemiga de novedades . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([tokenizer.decode(x).replace(' ', '') for x in inputs['input_ids'][100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the `labels` of the inputs are defined as a copy of the `input_ids`, and some percentage of the inputs are masked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False,  True,  ..., False, False, False],\n",
       "        [False, False,  True,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "mask_arr = (rand < MASK_PROB) * (inputs.input_ids != CLS_TOKEN) * (inputs.input_ids != SEP_TOKEN) * (inputs.input_ids != PAD_TOKEN)\n",
    "mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    4,  1098,  1000,  ...,     1,     1,     1],\n",
       "        [    4,  1413,     0,  ...,     1,     1,     1],\n",
       "        [    4,  1032,     0,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    4,  1057,  9447,  ...,     1,     1,     1],\n",
       "        [    4,  1032,  3876,  ...,     1,     1,     1],\n",
       "        [    4,  1265, 30957,  ...,     1,     1,     1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = mask_arr.bool()\n",
    "indices = mask.nonzero(as_tuple=False)\n",
    "inputs.input_ids[indices[:, 0], indices[:, 1]] = MASK_TOKEN \n",
    "\n",
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train with Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the processing, and efficiency for training the model, the `inputs` object will be converted to a Dataset object, and there will be defined an optimizer for the training. Also, if there's available GPU, the model will be moved to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OldSpanishDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.encodings.input_ids.shape[0]\n",
    "\n",
    "dataset = OldSpanishDataset(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "optim = Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture training_output\n",
    "%%time\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "        \n",
    "        if epoch % 1 == 0 and step % 100 == 0:\n",
    "            print(f\"Epoch {epoch} | step {step:03d} Loss: {loss.item()} \")\n",
    "\n",
    "    print(f\"Epoch {epoch} | step {step:03d} Loss: {loss.item()} [end]\")\n",
    "\n",
    "print(\"Training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save the model into a file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(MODEL_SAVE_PATH)\n",
    "model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
